ml_switcheroo.plugins.mlx_extras
================================

.. py:module:: ml_switcheroo.plugins.mlx_extras

.. autoapi-nested-parse::

   Plugin for MLX Ecosystem Mapping.

   Handles:
   1. Compilation: `@torch.compile` -> `@mx.compile`.
   2. Eager Evaluation: `torch.cuda.synchronize()` -> Warning/No-op.



Functions
---------

.. autoapisummary::

   ml_switcheroo.plugins.mlx_extras.transform_compiler
   ml_switcheroo.plugins.mlx_extras.transform_synchronize


Module Contents
---------------

.. py:function:: transform_compiler(node: Union[libcst.Decorator, libcst.Call], ctx: ml_switcheroo.core.hooks.HookContext) -> libcst.CSTNode

   Hook: Maps JIT compilation decorators.

   Triggers: Operations mapped with `requires_plugin: "mlx_compiler"`.

   Transformation:
       Input:  `@torch.compile(fullgraph=True, dynamic=True)`
       Output: `@mx.compile` (stripping incompatible kwargs).

   Decoupling:
       Looks up the `Compile` operation API in semantics (e.g. `mlx.core.compile`).


.. py:function:: transform_synchronize(node: libcst.Call, ctx: ml_switcheroo.core.hooks.HookContext) -> libcst.CSTNode

   Hook: Maps barrier synchronization to a warning.

   MLX is lazy, but `torch.cuda.synchronize()` implies a global device barrier.
   Equivalent `mx.eval()` requires arguments. Since we cannot infer state variables here,
   we replace the call with a print statement to alert the user.


