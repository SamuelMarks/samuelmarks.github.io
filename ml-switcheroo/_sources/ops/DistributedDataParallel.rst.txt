DistributedDataParallel
=======================

Implement distributed data parallelism based on ``torch.distributed`` at module level.

**Abstract Signature:**

``DistributedDataParallel(self, module, device_ids, output_device, dim, broadcast_buffers, init_sync, process_group, bucket_cap_mb, find_unused_parameters, check_reduction, gradient_as_bucket_view, static_graph, delay_all_reduce_named_params, param_to_hook_all_reduce, mixed_precision, device_mesh, skip_all_reduce_unused_params)``

.. raw:: html

 
    <div class="op-tabs-container"> 
      <div class="op-tabs-nav"> 
        <button class="op-tab-btn active" onclick="openOpTab(event, 'PyTorch_0')">PyTorch</button> 
      </div> 
      <div class="op-tabs-content"> 
        <div id="PyTorch_0" class="op-tab-pane active"><h4>PyTorch</h4><div class="op-detail-row"><span class="label">API:</span> <code>torch.nn.parallel.distributed.DistributedDataParallel</code></div><div class="op-detail-row"><span class="label">Strategy:</span> <span>Direct Mapping</span></div><div class="op-detail-row"><a href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.distributed.DistributedDataParallel.html" target="_blank" class="op-doc-link">Official Docs â†—</a></div></div> 
      </div> 

      <!-- Load JS Logic only once per page ideally, but safe to exist globally --> 
      <script src="../_static/op_tabs.js"></script> 
    </div> 

