ml_switcheroo.plugins.mlx_optimizers
====================================

.. py:module:: ml_switcheroo.plugins.mlx_optimizers

.. autoapi-nested-parse::

   Plugin for MLX Optimizer translation.

   Handles impedance mismatches for Functional Optimizers.



Functions
---------

.. autoapisummary::

   ml_switcheroo.plugins.mlx_optimizers.transform_mlx_optimizer_init
   ml_switcheroo.plugins.mlx_optimizers.transform_mlx_optimizer_step
   ml_switcheroo.plugins.mlx_optimizers.transform_mlx_zero_grad


Module Contents
---------------

.. py:function:: transform_mlx_optimizer_init(node: libcst.Call, ctx: ml_switcheroo.core.hooks.HookContext) -> libcst.Call

   Hook: Transforms Optimizer Constructor.

   1. Renames API based on context lookup or dynamic class construction.
   2. Strips parameter argument (Arg 0).
   3. Renames `lr` -> `learning_rate`.


.. py:function:: transform_mlx_optimizer_step(node: libcst.Call, ctx: ml_switcheroo.core.hooks.HookContext) -> Union[libcst.Call, libcst.FlattenSentinel]

   Hook: Transforms `optimizer.step()` into an EscapeHatch pattern.
   Functional optimizers (like MLX/Optax) require explicit update calls `opt.update(model, state)`.


.. py:function:: transform_mlx_zero_grad(node: libcst.Call, ctx: ml_switcheroo.core.hooks.HookContext) -> libcst.CSTNode

   Hook: Transforms `optimizer.zero_grad()` into `None` (No-Op).


