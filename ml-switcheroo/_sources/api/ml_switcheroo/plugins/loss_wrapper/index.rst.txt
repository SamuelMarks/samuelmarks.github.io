ml_switcheroo.plugins.loss_wrapper
==================================

.. py:module:: ml_switcheroo.plugins.loss_wrapper

.. autoapi-nested-parse::

   Plugin for Loss Reduction Semantics.

   Addresses the mismatch between:

   1. PyTorch: `loss = F.cross_entropy(..., reduction='mean')` (Scalar output by default).
   2. Functional Frameworks (JAX/Optax): `loss = optax.softmax_cross_entropy(x, y)`
      (Vector output per batch element).

   JAX libraries typically return the loss *per sample* to support vmap/pmap flexibility.
   PyTorch defaults to averaging (`mean`) immediately.

   Transformation:

   1. Detects `reduction` keyword argument.
   2. Strips the argument (as Optax/JAX funcs don't usually accept it).
   3. Wraps the function call in `Mean(x)` or `Sum(x)`. This step dynamically looks up
      the "Mean" or "Sum" API from the Semantic Knowledge Base, supporting any target
      framework definition (e.g. `tf.reduce_mean`, `jnp.mean`, `mx.mean`).
   4. If `reduction='none'`, leaves the vector output alone.



Functions
---------

.. autoapisummary::

   ml_switcheroo.plugins.loss_wrapper.transform_loss_reduction


Module Contents
---------------

.. py:function:: transform_loss_reduction(node: libcst.Call, ctx: ml_switcheroo.core.hooks.HookContext) -> libcst.CSTNode

   Hook: Wraps loss functions to apply reduction.

   Trigger: Operations mapped with `requires_plugin: "loss_reduction"`.
   Target: Frameworks requiring explicit reduction (JAX, Flax).

   :param node: The original CST Call node.
   :param ctx: HookContext for API lookup.

   :returns: Transformed Call node (wrapped or unwrapped).


