ml_switcheroo.plugins.lifecycle_flags
=====================================

.. py:module:: ml_switcheroo.plugins.lifecycle_flags

.. autoapi-nested-parse::

   Plugin for Lifecycle Flags (Training/Eval Modes).

   This plugin bridges the gap between PyTorch's Object-Oriented state management
   and the Functional patterns favored by JAX/Flax and Keras.

   Transforms:
   - `model.train()`       -> `training = True`
   - `model.train(False)`  -> `training = False`
   - `model.eval()`        -> `training = False`

   Usage:
     - Automatic for loops targetting JAX or Keras 3.
     - Useful for conditioning `Dropout` or `BatchNorm` layers downstream.



Functions
---------

.. autoapisummary::

   ml_switcheroo.plugins.lifecycle_flags.convert_lifecycle_flags


Module Contents
---------------

.. py:function:: convert_lifecycle_flags(node: libcst.CSTNode, ctx: ml_switcheroo.core.hooks.HookContext) -> Union[libcst.CSTNode, libcst.RemovalSentinel]

   Rewrites module state calls to functional flag assignments.

   Hooks into `TopLevel` or `Expr` nodes to replace statements.
   We target `Expr` (Expression Statement) because converting a `Call` expression
   inside another expression (e.g. `func(model.train())`) to an Assignment is invalid syntax.

   Target logic:
   1. Rewrite `*.train()` -> `training = True`
   2. Rewrite `*.eval()`  -> `training = False`


