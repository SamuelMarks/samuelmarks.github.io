EinsumDense
===========

A layer that uses `einsum` as the backing computation.

This layer can perform einsum calculations of arbitrary dimensionality.

Args:
    equation: An equation describing the einsum to perform.
        This equation must be a valid einsum string of the form
        `ab,bc->ac`, `...ab,bc->...ac`, or
        `ab...,bc->ac...` where 'ab', 'bc', and 'ac' can be any valid einsum
        axis expression sequence.
    output_shape: The expected shape of the output tensor
        (excluding the batch dimension and any dimensions
        represented by ellipses). You can specify `None` for any dimension
        that is unknown or can be inferred from the input shape.
    activation: Activation function to use. If you don't specify anything,
        no activation is applied
        (that is, a "linear" activation: `a(x) = x`).
    bias_axes: A string containing the output dimension(s)
        to apply a bias to. Each character in the `bias_axes` string
        should correspond to a character in the output portion
        of the `equation` string.
    kernel_initializer: Initializer for the `kernel` weights matrix.
    bias_initializer: Initializer for the bias vector.
    kernel_regularizer: Regularizer function applied to the `kernel` weights
        matrix.
    bias_regularizer: Regularizer function applied to the bias vector.
    kernel_constraint: Constraint function applied to the `kernel` weights
        matrix.
    bias_constraint: Constraint function applied to the bias vector.
    lora_rank: Optional integer. If set, the layer's forward pass
        will implement LoRA (Low-Rank Adaptation)
        with the provided rank. LoRA sets the layer's kernel
        to non-trainable and replaces it with a delta over the
        original kernel, obtained via multiplying two lower-rank
        trainable matrices
        (the factorization happens on the last dimension).
        This can be useful to reduce the
        computation cost of fine-tuning large dense layers.
        You can also enable LoRA on an existing
        `EinsumDense` layer by calling `layer.enable_lora(rank)`.
     lora_alpha: Optional integer. If set, this parameter scales the
        low-rank adaptation delta (computed as the product of two lower-rank
        trainable matrices) during the forward pass. The delta is scaled by
        `lora_alpha / lora_rank`, allowing you to fine-tune the strength of
        the LoRA adjustment independently of `lora_rank`.
    **kwargs: Base layer keyword arguments, such as `name` and `dtype`.

Examples:

**Biased dense layer with einsums**

This example shows how to instantiate a standard Keras dense layer using
einsum operations. This example is equivalent to
`keras.layers.Dense(64, use_bias=True)`.

>>> layer = keras.layers.EinsumDense("ab,bc->ac",
...                                       output_shape=64,
...                                       bias_axes="c")
>>> input_tensor = keras.Input(shape=[32])
>>> output_tensor = layer(input_tensor)
>>> output_tensor.shape
(None, 64)

**Applying a dense layer to a sequence**

This example shows how to instantiate a layer that applies the same dense
operation to every element in a sequence. Here, the `output_shape` has two
values (since there are two non-batch dimensions in the output); the first
dimension in the `output_shape` is `None`, because the sequence dimension
`b` has an unknown shape.

>>> layer = keras.layers.EinsumDense("abc,cd->abd",
...                                       output_shape=(None, 64),
...                                       bias_axes="d")
>>> input_tensor = keras.Input(shape=[32, 128])
>>> output_tensor = layer(input_tensor)
>>> output_tensor.shape
(None, 32, 64)

**Applying a dense layer to a sequence using ellipses**

This example shows how to instantiate a layer that applies the same dense
operation to every element in a sequence, but uses the ellipsis notation
instead of specifying the batch and sequence dimensions.

Because we are using ellipsis notation and have specified only one axis, the
`output_shape` arg is a single value. When instantiated in this way, the
layer can handle any number of sequence dimensions - including the case
where no sequence dimension exists.

>>> layer = keras.layers.EinsumDense("...x,xy->...y",
...                                       output_shape=64,
...                                       bias_axes="y")
>>> input_tensor = keras.Input(shape=[32, 128])
>>> output_tensor = layer(input_tensor)
>>> output_tensor.shape
(None, 32, 64)

**Abstract Signature:**

``EinsumDense(equation, output_shape, activation, bias_axes, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, kernel_constraint, bias_constraint, lora_rank, lora_alpha, gptq_unpacked_column_size, quantization_config, kwargs)``

.. raw:: html

 
    <div class="op-tabs-container"> 
      <div class="op-tabs-nav"> 
        <button class="op-tab-btn active" onclick="openOpTab(event, 'Keras_0')">Keras</button><button class="op-tab-btn " onclick="openOpTab(event, 'TensorFlow_1')">TensorFlow</button> 
      </div> 
      <div class="op-tabs-content"> 
        <div id="Keras_0" class="op-tab-pane active"><h4>Keras</h4><div class="op-detail-row"><span class="label">API:</span> <code>keras.layers.EinsumDense</code></div><div class="op-detail-row"><span class="label">Strategy:</span> <span>Direct Mapping</span></div><div class="op-detail-row"><a href="https://keras.io/search.html?q=keras.layers.EinsumDense" target="_blank" class="op-doc-link">Official Docs ↗</a></div></div><div id="TensorFlow_1" class="op-tab-pane "><h4>TensorFlow</h4><div class="op-detail-row"><span class="label">API:</span> <code>keras.layers.EinsumDense</code></div><div class="op-detail-row"><span class="label">Strategy:</span> <span>Direct Mapping</span></div><div class="op-detail-row"><a href="https://www.tensorflow.org/api_docs/python/keras/layers/EinsumDense" target="_blank" class="op-doc-link">Official Docs ↗</a></div></div> 
      </div> 

      <!-- Load JS Logic only once per page ideally, but safe to exist globally --> 
      <script src="../_static/op_tabs.js"></script> 
    </div> 

